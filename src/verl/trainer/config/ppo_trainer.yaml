# Format checks enforced on CI:
# 1. Comments must appear above each field.
# 2. There must be a blank line between each field.
# 3. Inline comments (after a field on the same line) are not allowed.
# 4. Indentation level is respected for nested fields.

# dataset config
data:
  # Name of the benchmark to use in libero.
  task_suite_name: libero_10
  
  # Number of trials per task.
  num_trials_per_task: 50

  # Whether to use shared memory for data loading.
  use_shm: False

  # Maximum prompt length. All prompts will be left-padded to this length.
  # An error will be reported if the length is too long.
  max_prompt_length: 672

  # Maximum response length. Rollout in RL algorithms (e.g. PPO) generates up to this length.
  max_response_length: 360

  # Batch size sampled for one training iteration of different RL algorithms.
  train_batch_size: 64

  # Batch size used during validation. Can be null.
  val_batch_size: 480

  # Number of samples to generate for each task.
  n_samples: 1

  # Whether to shuffle the data in the dataloader.
  shuffle: True

  # Whether to shuffle the validation set.
  validation_shuffle: True

  filter_accuracy: True

  filter_truncated: False

  accuracy_lower_bound: 0.1

  accuracy_upper_bound: 0.9

  oversample_factor: 1.0 # Sample more responses than the batch size


# config for actor, rollout and reference model
actor_rollout_ref:

  # Whether it's a hybrid engine, currently only supports hybrid engine
  hybrid_engine: true

  # common configs for the model
  model:
    action_dim_len: 7

    action_chunks_len: 10

    # Huggingface model path. This can be either local path or HDFS path.
    path: your_model_path/sft_cot

    # Whether to use shared memory (SHM) for accelerating the loading of model weights
    use_shm: false
    
    # Enable gradient checkpointing for actor
    enable_gradient_checkpointing: false
    
    # Enable activation offloading for actor
    enable_activation_offload: false
    
    # Set to positive value to enable LoRA (e.g., 32)
    lora_rank: 0
    
    # LoRA scaling factor
    lora_alpha: 32
    
    # Target modules to apply LoRA. Options: "all-linear" or 
    # [q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj]
    target_modules: all-linear

  # configs for the actor
  actor:

    num_images_in_input: 2

    traj_mini_batch_size: 8

    action_chunks_len: ${actor_rollout_ref.model.action_chunks_len}

    action_dim_len: ${actor_rollout_ref.model.action_dim_len}

    cot_length: ${data.max_response_length}
  
    # fsdp, fsdp2 or megatron. fsdp backend used here.
    strategy: fsdp
    
    # Split each sample into sub-batches of this size for PPO
    ppo_mini_batch_size: 8
    
    # [Deprecated] Global micro batch size
    ppo_micro_batch_size: null
    
    # Local per-GPU micro batch size
    ppo_micro_batch_size_per_gpu: null
    
    # Gradient clipping for actor updates
    grad_clip: 1.0
    
    # PPO clip ratio
    clip_ratio: 0.2
    
    # Lower bound for asymmetric clipping (used in dual-clip PPO)
    clip_ratio_low: 0.2
    
    # Upper bound for asymmetric clipping (used in dual-clip PPO)
    clip_ratio_high: 0.28
    
    # Constant C in Dual-clip PPO; clips when advantage < -C
    clip_ratio_c: 3.0
    
    # Loss aggregation mode: "token-mean", "seq-mean-token-sum", or "seq-mean-token-mean" or "seq-mean-token-sum-norm"
    loss_agg_mode: token-mean
    
    # Entropy regularization coefficient in PPO loss
    entropy_coeff: 0.001

    # Whether to use KL loss instead of KL reward penalty. True for GRPO
    use_kl_loss: false
    
    # Whether to use torch.compile()
    use_torch_compile: true

    # KL loss coefficient when use_kl_loss is enabled. For GRPO
    kl_loss_coef: 0.001
    
    # Type of KL divergence loss. Options: "kl"(k1), "abs", "mse"(k2), "low_var_kl"(k3), "full"
    kl_loss_type: low_var_kl
    
    # Number of PPO epochs per batch
    ppo_epochs: 1
    
    # Shuffle training data across PPO epochs
    shuffle: false

    # checkpoint configs
    checkpoint:
    
      # What to include in saved checkpoints
      # with 'hf_model' you can save whole model as hf format, now only use sharded model checkpoint to save space
      contents: ['model', 'optimizer', 'extra', 'hf_model']

    # optimizer configs
    optim:
    
      # Learning rate
      lr: 1e-6
      
      # Warmup steps; negative value delegates to lr_warmup_steps_ratio
      lr_warmup_steps: -1
      
      # Warmup steps ratio (used if lr_warmup_steps is negative)
      lr_warmup_steps_ratio: 0.0
      
      # Minimum LR ratio for cosine schedule
      min_lr_ratio: 0.0
      
      # Number of cosine cycles in LR schedule
      num_cycles: 0.5
      
      # LR warmup style: "constant" or "cosine"
      warmup_style: constant
      
      # Total training steps (must be overridden at runtime)
      total_training_steps: -1
      
      # Weight decay
      weight_decay: 0.01

    # configs for FSDP
    fsdp_config:

      # policy for wrapping the model
      wrap_policy:
      
        # Minimum number of parameters to trigger wrapping a layer with FSDP
        min_num_params: 5e7
        
      # Whether to offload model parameters to CPU (trades speed for memory)
      param_offload: false
      
      # Whether to offload optimizer state to CPU
      optimizer_offload: false
      
      # Only for FSDP2: offload param/grad/optimizer during train
      offload_policy: false
      
      # Only for FSDP2: Reshard after forward pass to reduce memory footprint
      reshard_after_forward: true
      
      # Number of GPUs in each FSDP shard group; -1 means auto
      fsdp_size: -1

      model_dtype: bfloat16

  # Reference model config.
  # Reference model will be enabled when actor.use_kl_loss or/and algorithm.use_kl_in_reward is/are True.
  ref:

    action_chunks_len: ${actor_rollout_ref.model.action_chunks_len}
    
    # actor_rollout_ref.ref: FSDP config same as actor. For models larger than 7B, itâ€™s recommended to turn on offload for ref by default
    strategy: fsdp

    # config for FSDP strategy
    fsdp_config:
      model_dtype: bfloat16

      # whether to offload parameters in FSDP
      param_offload: False

      # whether to perform reshard after model forward to save memory.
      # only for fsdp2, [True, False, int between 1 and fsdp_size]
      reshard_after_forward: True

      # the wrap policy for FSDP model
      wrap_policy:

        # minimum number of params in a wrapped module
        min_num_params: 0

    # whether to enable torch.compile
    use_torch_compile: ${actor_rollout_ref.actor.use_torch_compile}

    # [Will be deprecated, use log_prob_micro_batch_size_per_gpu]
    # The batch size for one forward pass in the computation of log_prob. Global batch size.
    log_prob_micro_batch_size: null

    # The batch size for one forward pass in the computation of log_prob. Local batch size per GPU.
    log_prob_micro_batch_size_per_gpu: null

  # Rollout model config.
  rollout:

    experiment_name: ${trainer.experiment_name}

    micro_batch_size: 4

    val_micro_batch_size: 31

    task_suite_name: ${data.task_suite_name}

    num_steps_wait: 10

    pretrained_checkpoint: ${actor_rollout_ref.model.path}

    center_crop: True

    seed: 429

    cot_temperature: 1.0

    action_temperature: 1.0

    action_dim_len: ${actor_rollout_ref.model.action_dim_len}

    action_chunks_len: ${actor_rollout_ref.model.action_chunks_len}

    num_images_in_input: ${actor_rollout_ref.actor.num_images_in_input}

    # actor_rollout_ref.rollout.name: hf/vllm/sglang.
    name: hf

    # sync: LLM, async: AsyncLLM
    mode: sync

    # Top-k sampling parameter. -1 for vLLM rollout, 0 for HF rollout.
    top_k: 0

    # Top-p sampling parameter. Default 1.0.
    top_p: 1

    # typically the same as data max prompt length
    max_prompt_length: ${data.max_prompt_length}
    
    # typically the same as data max response length
    response_length: ${data.max_response_length}

    # Rollout model parameters type. Align with actor model's FSDP/Megatron type.
    dtype: bfloat16

    # [Will be deprecated, use log_prob_micro_batch_size_per_gpu] The batch size for one forward pass in the computation of log_prob. Global batch size.
    log_prob_micro_batch_size: null

    # The batch size for one forward pass in the computation of log_prob. Local batch size per GPU.
    log_prob_micro_batch_size_per_gpu: null

    # Whether to sample during training rollout. False uses greedy sampling.
    do_sample: True

    ### FOR VLLM ROLLOUT
    limit_images: 3
    # Fraction of GPU memory used by vLLM/SGLang for KV cache.
    gpu_memory_utilization: 0.9

    # https://arxiv.org/abs/2410.21236
    use_fire_sampling: False

    # Whether to ignore EOS and continue generating after EOS is hit.
    ignore_eos: False

    # Whether to disable CUDA graph. Default True to allow cache freeing.
    enforce_eager: True

    # Whether to free engine KVCache after generation. Set enforce_eager=True when enabled.
    free_cache_engine: True

    # Which loader to use for rollout model weights: dummy_dtensor, hf, megatron, etc.
    # safetensors (for huge model, and set use_shm=True); dummy_dtensor: randomly init model weight
    load_format: dummy_dtensor

    # for huge model, layered summon can save memory (prevent OOM) but make it slower
    layered_summon: False

    # TP size for rollout. Only effective for vLLM.
    tensor_model_parallel_size: 1

    # max number of tokens in a batch
    max_num_batched_tokens: 8192

    # max length for rollout
    max_model_len: null

    # max length of sequences
    max_num_seqs: 1024

    # disable logging statistics
    disable_log_stats: True

    # may get higher throughput when set to True. When activated, Please increase max_num_batched_tokens or decrease max_model_len.
    enable_chunked_prefill: True

    # number of responses (i.e. num sample times). > 1 for grpo
    n: 1

    # Extra inference engine arguments (vllm, sglang).
    engine_kwargs:

      # for vllm 
      vllm:

        # Swap space (in GB) used by inference engine. null uses default (e.g., 4 GB).
        swap_space: null

      # for sglang
      sglang:

        # The attention backend for sglang engine. Options: flashinfer, triton, flashmla, null for default.
        attention_backend: null

    # Sampling parameters used during validation.
    val_kwargs:

      # sampling parameters for validation
      # Top-k sampling parameter. -1 for vLLM rollout, 0 for HF rollout.
      top_k: -1

      # Top-p sampling parameter. Default 1.0.
      top_p: 1.0

      # whether to repeat n times for validation
      n: 1

      # Whether to sample during training rollout. False uses greedy sampling.
      do_sample: False

# verifier is more than source of reward. so it should be independent
verifier:
  acc_coef: 2
  format_coef: 1

# config for the algorithm
algorithm:

  # Discount factor for future rewards
  gamma: 1.0

  # Trade-off between bias and variance in the GAE estimator
  lam: 1.0

  # Advantage estimator type: "gae", "grpo", "reinforce_plus_plus", etc.
  adv_estimator: grpo

  # Whether to normalize advantages by std (specific to GRPO)
  norm_adv_by_std_in_grpo: True

  # Whether to enable in-reward KL penalty
  use_kl_in_reward: False

  # How to estimate KL divergence: "kl", "abs", "mse", "low_var_kl", or "full"
  kl_penalty: kl

  # KL control configuration
  kl_ctrl:

    # KL control type: "fixed" or "adaptive"
    type: fixed

    # Initial coefficient for KL penalty
    kl_coef: 0.001

    # Horizon value for adaptive controller (if enabled)
    horizon: 10000

    # Target KL divergence (used for adaptive controller)
    target_kl: 0.1

  # Whether to enable preference feedback PPO
  use_pf_ppo: False

  # Preference feedback PPO settings
  pf_ppo:

    # Method for reweighting samples: "pow", "max_min", or "max_random"
    reweight_method: pow

    # Power used for weight scaling in "pow" method
    weight_pow: 2.0

# config for the trainer
trainer:

  val_only: False

  update_warmup: 0

  # runtime environment file path
  runtime_env: none

  # Whether to balance batch sizes across distributed workers
  balance_batch: True

  # Number of epochs in training
  total_epochs: 30

  # Total training steps (can be set explicitly or derived from epochs)
  total_training_steps: null

  # Project name for experiment tracking (e.g., wandb)
  project_name: verl_examples

  # Experiment name for run identification in tracking tools
  experiment_name: gsm8k

  # Logging backends to use: "console", "wandb", etc.
  logger: [ 'console', 'wandb' ]

  # Number of generations to log during validation
  log_val_generations: 0

  # Directory for logging rollout data; no dump if null
  rollout_data_dir: null

  # Directory for logging validation data; no dump if null
  validation_data_dir: null

  # Number of nodes used in the training
  nnodes: 1

  # Number of GPUs per node
  n_gpus_per_node: 8

  # Save frequency (by iteration) for model checkpoints
  save_freq: -1

  # Resume mode: "auto", "disable", or "resume_path"
  # "auto": resume from last checkpoint if available
  # "disable": start from scratch
  # "resume_path": resume from a user-defined path
  resume_mode: auto

  # Path to resume training from (only used when resume_mode is "resume_path")
  resume_from_path: null

  # Whether to run validation before training begins
  val_before_train: False

  # Validation frequency (in training iterations)
  test_freq: -1

  # Number of iterations to warm up the critic before updating policy
  critic_warmup: 0

  # Default path to distributed filesystem for saving checkpoints
  default_hdfs_dir: null

  # Whether to delete local checkpoints after loading
  del_local_ckpt_after_load: False

  # Default local directory for saving checkpoints
  default_local_dir: checkpoints/${trainer.project_name}/${trainer.experiment_name}

  # Maximum number of actor checkpoints to keep
  max_actor_ckpt_to_keep: null

  # Maximum number of critic checkpoints to keep
  max_critic_ckpt_to_keep: null

  # Timeout (in seconds) for Ray worker to wait for registration
  ray_wait_register_center_timeout: 300

  # Device to run training on (e.g., "cuda", "cpu")
  device: cuda

# configs related to ray initialization
ray_init:

  # Number of CPUs for Ray. Use a fixed number instead of null when using SLURM.
  num_cpus: null

  # Path to save Ray timeline JSON for performance profiling
  timeline_json_file: null
